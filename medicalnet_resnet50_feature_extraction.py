# -*- coding: utf-8 -*-
"""MedicalNet_ResNet50_Feature_Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NTDWEUmvEFDOpOnAMrVTNnxNbkVhfuXY
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!git clone https://github.com/Tencent/MedicalNet

## Chunk of code below is borrowed from the MedicalNet Repo to be adapted for this project (link to MedicalNet in README)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math
from functools import partial

__all__ = [
    'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
    'resnet152', 'resnet200'
]


def conv3x3x3(in_planes, out_planes, stride=1, dilation=1):
    # 3x3x3 convolution with padding
    return nn.Conv3d(
        in_planes,
        out_planes,
        kernel_size=3,
        dilation=dilation,
        stride=stride,
        padding=dilation,
        bias=False)


def downsample_basic_block(x, planes, stride, no_cuda=False):
    out = F.avg_pool3d(x, kernel_size=1, stride=stride)
    zero_pads = torch.Tensor(
        out.size(0), planes - out.size(1), out.size(2), out.size(3),
        out.size(4)).zero_()
    if not no_cuda:
        if isinstance(out.data, torch.cuda.FloatTensor):
            zero_pads = zero_pads.cuda()

    out = Variable(torch.cat([out.data, zero_pads], dim=1))

    return out


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3x3(inplanes, planes, stride=stride, dilation=dilation)
        self.bn1 = nn.BatchNorm3d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3x3(planes, planes, dilation=dilation)
        self.bn2 = nn.BatchNorm3d(planes)
        self.downsample = downsample
        self.stride = stride
        self.dilation = dilation

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm3d(planes)
        self.conv2 = nn.Conv3d(
            planes, planes, kernel_size=3, stride=stride, dilation=dilation, padding=dilation, bias=False)
        self.bn2 = nn.BatchNorm3d(planes)
        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm3d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride
        self.dilation = dilation

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self,
                 block,
                 layers,
                 sample_input_D,
                 sample_input_H,
                 sample_input_W,
                 num_seg_classes,
                 shortcut_type='B',
                 no_cuda = False):
        self.inplanes = 64
        self.no_cuda = no_cuda
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv3d(
            1,
            64,
            kernel_size=7,
            stride=(2, 2, 2),
            padding=(3, 3, 3),
            bias=False)

        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)
        self.layer2 = self._make_layer(
            block, 128, layers[1], shortcut_type, stride=2)
        self.layer3 = self._make_layer(
            block, 256, layers[2], shortcut_type, stride=1, dilation=2)
        self.layer4 = self._make_layer(
            block, 512, layers[3], shortcut_type, stride=1, dilation=4)
        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))

        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')
            elif isinstance(m, nn.BatchNorm3d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1, dilation=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            if shortcut_type == 'A':
                downsample = partial(
                    downsample_basic_block,
                    planes=planes * block.expansion,
                    stride=stride,
                    no_cuda=self.no_cuda)
            else:
                downsample = nn.Sequential(
                    nn.Conv3d(
                        self.inplanes,
                        planes * block.expansion,
                        kernel_size=1,
                        stride=stride,
                        bias=False), nn.BatchNorm3d(planes * block.expansion))

        layers = []
        layers.append(block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes, dilation=dilation))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)  # Apply global average pooling
        x = torch.flatten(x, 1)
        #x = self.conv_seg(x)

        return x


def resnet50(**kwargs):
    """Constructs a ResNet-50 model.
    """
    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
    return model



## End of borrowed code

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import nibabel as nib
import numpy as np
from MedicalNet.models.resnet import ResNet, Bottleneck, resnet50
from collections import OrderedDict
import os

# Define the ResNet Feature Extractor
class ResNetFeatureExtractor(ResNet):
    def __init__(self, block, layers, input_D, input_H, input_W, num_seg_classes, shortcut_type='B', no_cuda=False):
        super(ResNetFeatureExtractor, self).__init__(block, layers, input_D, input_H, input_W, num_seg_classes, shortcut_type, no_cuda)
        # Remove the final segmentation layer for feature extraction
        self.conv_seg = None
        # Add the average pooling layer
        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        features = self.layer4(x)  # Extract features from layer4
        features = self.avgpool(features)  # Apply average pooling
        features = torch.flatten(features, 1)  # Flatten to get the feature vector
        return features

# Initialize model with the desired dimensions
model = ResNetFeatureExtractor(
    block=Bottleneck,
    layers=[3, 4, 6, 3],
    input_D=256,  # Depth
    input_H=256,  # Height
    input_W=256,  # Width
    num_seg_classes=1,
    shortcut_type='B',
    no_cuda=True
)

# Load pre-trained weights
pretrain_path = "/content/drive/MyDrive/internship_KU/MedicalNet_files/pretrain/resnet_50_23dataset.pth"
pretrain = torch.load(pretrain_path)
pretrained_dict = pretrain['state_dict']
new_state_dict = OrderedDict()
for k, v in pretrained_dict.items():
    name = k[7:]  # Strip 'module.' prefix if necessary
    new_state_dict[name] = v

model.load_state_dict(new_state_dict, strict=False)

# Define your dataset and dataloader
class NiftiDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.files = [f for f in os.listdir(data_dir) if f.endswith('.nii') or f.endswith('.nii.gz')]

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = os.path.join(self.data_dir, self.files[idx])
        img = nib.load(file_path).get_fdata()
        img = img / 255
        img = np.expand_dims(img, axis=0)  # Add channel dimension
        img = torch.tensor(img, dtype=torch.float32)  # Convert to tensor and ensure dtype is float32
        if self.transform:
            img = self.transform(img)
        return img

dataset = NiftiDataset(data_dir="/content/drive/MyDrive/internship_KU/Images-FreeSurfer/")
dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

# Perform feature extraction
model.eval()
features = []

for inputs in dataloader:
    inputs = inputs.float()  # Ensure inputs are of type float
    with torch.no_grad():
        outputs = model(inputs)  # Extract features
        features.append(outputs)

# Combine features from all batches
features = torch.cat(features, dim=0)  # Concatenate along the batch dimension
print(features.shape)  # Print output tensor shape

features_np = features.numpy()

# Save the NumPy array as a CSV file
np.savetxt('features.csv', features_np, delimiter=',')

# Using MedicalNet for feature extraction

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import nibabel as nib
import numpy as np
from collections import OrderedDict
import os

# Initialize model with the desired dimensions
model = resnet50(
    sample_input_D=256,  # Depth
    sample_input_H=256,  # Height
    sample_input_W=256,  # Width
    shortcut_type='B',
    num_seg_classes = 1,
    no_cuda=True
)

# Load pre-trained weights
pretrain_path = "/content/drive/MyDrive/internship_KU/MedicalNet_files/pretrain/resnet_50_23dataset.pth"
pretrain = torch.load(pretrain_path)
pretrained_dict = pretrain['state_dict']
new_state_dict = OrderedDict()
for k, v in pretrained_dict.items():
    name = k[7:]  # Strip 'module.' prefix if necessary
    new_state_dict[name] = v

model.load_state_dict(new_state_dict, strict=False)

# Define your dataset and dataloader
class NiftiDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.files = [f for f in os.listdir(data_dir) if f.endswith('.nii') or f.endswith('.nii.gz')]

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = os.path.join(self.data_dir, self.files[idx])
        img = nib.load(file_path).get_fdata()
        img = img / 255
        img = np.expand_dims(img, axis=0)  # Add channel dimension
        img = torch.tensor(img, dtype=torch.float32)  # Convert to tensor and ensure dtype is float32
        if self.transform:
            img = self.transform(img)
        return img

dataset = NiftiDataset(data_dir="/content/drive/MyDrive/internship_KU/Images-FreeSurfer/")
dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

# Perform feature extraction
model.eval()
features = []

for inputs in dataloader:
    inputs = inputs.float()  # Ensure inputs are of type float
    with torch.no_grad():
        outputs = model(inputs)  # Extract features
        features.append(outputs)

# Combine features from all batches
features = torch.cat(features, dim=0)  # Concatenate along the batch dimension
print(features.shape)  # Print output tensor shape

features_np = features.numpy()

# Save the NumPy array as a CSV file
np.savetxt('features.csv', features_np, delimiter=',')